function [obj_opt,x_opt]= multiagent_opt(funchandle, varchoices)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OPTIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%experiment options
numKs=50;
numRuns = 1; 
stopEpoch=50; %If it hasn't improved after this many Epochs, stop
maxEpochs=250;
%agent options
alpha = 0.1;    % Learning rate
Qinit= 10000;   %Q-table initialization
TMin=0.1;
%plotting and workspace options
saveWorkspace = 1;
showConstraintViolation             = 0;
altplots                            =1;

rewardstruct='D';       %G, L, or D
rewardtype='DiffEst';    %learned, expImprovement, or DiffEst
availableactions=[1,0.5,0.2,0.1,0.05,-0.05,-0.1];
T=10;


%addpath('C:\Projects\GitHub\QuadrotorModel')
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

numVars = numel(varchoices);
% The discrete choices for the variables that give best performance
x_opt = uint8(zeros(numRuns,numVars)); 

numactions=numel(availableactions)*ones(1,numVars);

minobj = 10000*zeros(numRuns, 1);
completion = 0;

for r = 1:numRuns
    % Create the expectation of merit for the paremeters
    [expMerit] = create_expfuncs(varchoices,Qinit);
    values=create_expfuncs(numactions,-10000);
    
    % initializing best performance obtained
    bestobj(1)= 10000;
    obj=-10000;
    epochOfMax(r) = 0;
    e=1;
    converged=false;
    bestobjc=nan(1,maxEpochs);
    avgobjk=nan(1,maxEpochs);
    
    while converged==false
        e=e+1;
        
        bestobj(e)=bestobj(e-1);
        k=0;
        for k=1:numKs
            
            %choose actions based on learned values
            actions=choose_actions(values,T);
            %temps
            temps=availableactions(actions);
            
            % Have agents choose the values of each given design variable
            % x.
            x = choose_paramvals(expMerit, temps);

            % Calculate the objective function of the chosen design. Assign
            % that to the found merit of each paremeter value taken.
             obj=funchandle(x);
             foundMerit = ones(13, 1) * obj;
            
            % update the expected merit of each design variable given the
            % objective value calculated.
            [expMerit, learned,expimprovement,DiffEst] = update_merit(expMerit, foundMerit, alpha, x, 'best');
            agents_hist{r, e} = expMerit;
            
            rewards=calc_rewards(learned,expimprovement,DiffEst,rewardtype,rewardstruct);
            
            values=learn_values(values,actions,rewards,alpha);
            
            if any(learned)
            learndisp=' learned';
            else
                learndisp='.';
            end 
            disp([num2str(r,'%03.0f') ', ' num2str(e,'%03.0f') ', ' num2str(k,'%03.0f') ', obj=' num2str(obj, '%+10.2e\n') ', minf(x)=' num2str(bestobj(e), '%+10.2e\n') learndisp])
            if obj < bestobj(e) %&& all(constraints <= 0.01)
                bestobj(e) = obj;
                % Update record of best actions generated by the system
                x_opt(r,:) = x;
                obj_opt(r)=obj;
            end
            
        end

       if e>stopEpoch+1
            if bestobj(e)==bestobj(e-stopEpoch)
                converged=true;
            end
       end
       if e>maxEpochs
           converged=true;
       end
       
    end
    bestobjc(1:length(bestobj))=bestobj;
    bestobjhist(r,:)=bestobjc;
    avgobjhist(r,:)=avgobjk;
    clear bestobj
end

if ~exist('Saved Workspaces', 'dir')
    mkdir('Saved Workspaces');
end

generate_plots

end


